#!/bin/bash

### PERMISSIONS ###
umask 007

### DATABASE VARIABLES ###
hDB=protein_modeling
hPATH=/warehouse/tablespace/external/hive/${hDB}.db

### DEFINE BASE PATH ###
bpath=
if [ "$bpath" == "" ]; then
    bpath=$(cd -P "$(dirname "${BASH_SOURCE[0]}")" && pwd)
fi

### Load functions
source "$bpath/lib/ifx-shell/utils.sh"
#source "$bpath/lib/ifx-shell/log.sh"

### LOAD DIRECTORY PATHS ###

binp=$bpath/bin
docp=$bpath/docs
tblp=$bpath/tbl
libp=$bpath/lib

function check_tckt() {
    klist > /dev/null 2>&1 || {
        echo "$PROGRAM ERROR: Initialize Kerberos ticket granting ticket using kinit" >&2
        exit 1
    }
}
check_tckt

PROGRAM=SIMPLE

function time_stamp() {
	local t=$(date +"%Y-%m-%d %k:%M:%S")
	echo -e "[$t]\t$PROGRAM ::: $1"
}

function upload_table() {
    local file=$1
    local filename=$(basename "$file" .csv)
    local tbl=$2

    if [ -s "$file" ]; then
        time_stamp "Uploading $filename to $tbl."
        "$libp/hadoopBySSH/hput" "$file" "$hPATH/$tbl/${filename}.csv" || die "Upload failed for $filename."
        refresh "$tbl" die
    else
        warn "File '$file' does not exist."
    fi
}

if [ "$1" == "do" ]; then
    # 1-A: Upload reference files to Cloudera Hadoop schema
    time_stamp "BEGIN"
    upload_table "test.csv" glycosylation_distance || die
    #upload_table "$tblp/rank_limits.txt" rank_limits || die

    # 1-B: Refresh tables in Cloudera Hadoop 'protein_models' schema
    # NOTE: Strict order maintenance is required; several downstream tables depend on updates in prior tables (as indicated)
    hadoop_tables=(

        # tables
        #atomic_contacts
        glycosylation_distance
        #molecular_bonds
        #relative_site_interactions
        #rosetta_model
        #site_interactions

    )
fi
    #source "$bpath/lib/ifx-shell/utils.sh"

finish "END"
