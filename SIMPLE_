#!/bin/bash

### PERMISSIONS ###
umask 007
set -uo pipefail

source /groups/NCIRD/ID/apps/ifx-shell/kerberos.sh
### DATABASE VARIABLES ###
hDB=protein_modeling
hPATH=/warehouse/tablespace/external/hive/${hDB}.db

### DEFINE BASE PATH ###
bpath=
if [ "$bpath" == "" ]; then
    bpath=$(cd -P "$(dirname "${BASH_SOURCE[0]}")" && pwd)
fi
echo $bpath
### Load functions
source "$bpath/lib/ifx-shell/utils.sh"
#source "$bpath/lib/ifx-shell/log.sh"

### LOAD DIRECTORY PATHS ###

binp=$bpath/bin
docp=$bpath/docs
tblp=$bpath/tbl
libp=$bpath/lib
glyc_files=$bpath/tbl/*ASA_dist.csv
mol_contacts=$bpath/tbl/*getcontacts.csv
rosetta=$bpath/tbl/*scorefile.txt.csv

function check_tckt() {
    klist > /dev/null 2>&1 || {
        echo "$PROGRAM ERROR: Initialize Kerberos ticket granting ticket using kinit" >&2
        exit 1
    }
}
check_tckt

PROGRAM=SIMPLE

function time_stamp() {
        local t=$(date +"%Y-%m-%d %k:%M:%S")
        echo -e "[$t]\t$PROGRAM ::: $1"
}

function upload_table() {
    local file=$1
    local filename=$(basename "$file" .csv)
    local tbl=$2

    if [ -s "$file" ]; then
        time_stamp "Uploading $filename to $tbl."
        "$libp/hadoopBySSH/hput" "$file" "$hPATH/$tbl/$filename" || die "Upload failed for $file."
        refresh "$tbl" die
    else
        warn "File '$file' does not exist."
    fi
}

function refresh() {
    local tbl=$1
    local fnc=$2
    time_stamp "Refreshing '$tbl'"
    "$libp/hadoopBySSH/himpala" "refresh ${hDB}.$tbl; compute stats ${hDB}.$tbl;" > /dev/null || $fnc "Refresh of '$tbl' failed." 
}


if [ "$1" == "do" ]; then
    # 1-A: Upload reference files to Cloudera Hadoop schema
    time_stamp "BEGIN"
    for file in $glyc_files;
    do
        if [ -f $file ]; then
            filename=$(basename "$file")
            echo $filename
            upload_table $tblp/$filename glycosylation_distance || die
            mv $tblp/$filename $tblp/archive
        else
            warn "no files to upload to glycosylation distance table"
         fi
    done
    # 1-B: Refresh tables in Cloudera Hadoop 'protein_modeling' schema
    # NOTE: Strict order maintenance is required; several downstream tables depend on updates in prior tables (as indicated)      
    hadoop_tables=(

        # table
        glycosylation_distance
    )
fi
if [ "$1" == "do" ]; then
    # 1-A: Upload reference files to Cloudera Hadoop schema
    time_stamp "BEGIN"
    for file in $mol_contacts;
    do
        if [ -f $file ]; then
            filename=$(basename $file)
            echo $filename
            upload_table $tblp/$filename atomic_contacts || die
            mv $tblp/$filename $tblp/archive
        else
            warn "no files for upload to molecular contacts table"
        fi
    done
    # 1-B: Refresh tables in Cloudera Hadoop 'protein_modeling' schema
    # NOTE: Strict order maintenance is required; several downstream tables depend on updates in prior tables (as indicated)      
    hadoop_tables=(

        # table
        atomic_contacts
    )
fi
if [ "$1" == "do" ]; then
    # 1-A: Upload reference files to Cloudera Hadoop schema
    time_stamp "BEGIN"
    for file in $rosetta;
    do
        if [ -f $file ]; then
            filename=$(basename "$file")
            upload_table $tblp/$filename rosetta_model || die
            mv $tblp/$filename $tblp/archive
        else
            warn "no file to upload to energy table"
        fi
    done
    # 1-B: Refresh tables in Cloudera Hadoop 'protein_modeling' schema
    # NOTE: Strict order maintenance is required; several downstream tables depend on updates in prior tables (as indicated)      
    hadoop_tables=(

        # table
        rosetta_model
    )
fi
if [ "$1" == "do" ]; then
for file in $bpath/*.parquet;
do
    # 1-A: Upload reference files to Cloudera Hadoop schema
    time_stamp "BEGIN"
    filename=$(basename "$file")
    upload_table $bpath/$filename isolate_name || die
    # 1-B: Refresh tables in Cloudera Hadoop 'protein_modeling' schema
    # NOTE: Strict order maintenance is required; several downstream tables depend on updates in prior tables (as indic$    hadoop_tables=(
done
  hadoop_tables=(
        # table
        isolate_name
    )
fi
#    source "$bpath/lib/ifx-shell/utils.sh"

finish "END"
