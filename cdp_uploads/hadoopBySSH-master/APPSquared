#!/bin/bash
# Authors: Brian Mann (Code Founder/Lead Developer), Samuel Shepard, and Thomas Stark
# Referral System
# Purpose: Systematic update/refresh of "Referral System (REFSYS)" CDP Cloudera Hadoop 'ref_sys' tables

### PERMISSIONS ###
umask 007

### DATABASE VARIABLES ###
hDB=protein_models
hPATH=/warehouse/tablespace/external/hive/${hDB}.db

### DEFINE BASE PATH ###
bpath=
if [ "$bpath" == "" ]; then
    bpath=$(cd -P "$(dirname "${BASH_SOURCE[0]}")" && pwd)
fi

### Load functions
source "$bpath/lib/ifx-shell/utils.sh"
#source "$bpath/lib/ifx-shell/log.sh"

### LOAD DIRECTORY PATHS ###

binp=$bpath/bin
#docp=$bpath/docs
#rp=$bpath/r
#sqlp=$bpath/sql
tblp=$bpath/tbl
libp=$bpath/lib

### DETECT OS ###
OS="Linux"
command -v uname > /dev/null 2>&1 && OS=$(uname -s)
if [ "$OS" != "Linux" -a "$OS" != "Darwin" ]; then
    time_stamp "Your $OS OS not yet supported."
    exit 0
fi

### TEST OS/ASSIGN PROGRAMS ###
# stackoverflow.com/questions/592620/check-if-a-program-exists-from-a-bash-script
function check_prgm() {
    command -v "$1" > /dev/null 2>&1 || {
        echo "$PROGRAM ERROR: '$1' not found, add to the PATH or install." >&2
        exit 1
    }
}

if [ "$OS" == "Darwin" ]; then
    GREP=$binp/ggrep
else
    GREP="grep"
fi
check_prgm $GREP

# Check/re-initialize Kerberos ticket-granting-ticket (as needed) [REQUIRED: Cloudera CDP access]
function check_tckt() {
    klist > /dev/null 2>&1 || {
        echo "$PROGRAM ERROR: Initialize Kerberos ticket granting ticket using kinit" >&2
        exit 1
    }
}
check_tckt

### DEFINE FUNCTIONS ###
PROGRAM=APPSquared

function time_stamp() {
	local t=$(date +"%Y-%m-%d %k:%M:%S")
	echo -e "[$t]\t$PROGRAM ::: $1"
}

function warn() {
	local t=$(date +"%Y-%m-%d %k:%M:%S")
	echo -e "[$t] $PROGRAM WARNING :: $1" 1>&2
	return 1
}

function die() {
	local t=$(date +"%Y-%m-%d %k:%M:%S")
        echo -e "[$t]\t$PROGRAM ERROR :: $1" 1>&2
        exit 1
}

function dead() {
        echo -e "$PROGRAM ABORTED!" 1>&2
        exit 1
}

function print_help {
    cat << EOL

    Valid input format: ./$PROGRAM <FUNCTION>
       
    Functions:  do
                install
                stats
                update

EOL
    exit 0
}

# Upload TXT file(s) to Cloudera Hadoop (Impala) to update schema (INSERT OVERWRITE)
function upload_table() {
    local file=$1
    local filename=$(basename "$file" .csv)
    local tbl=$2

    if [ -s "$file" ]; then
        time_stamp "Uploading $filename to $tbl."
        "$libp/hadoopBySSH/hput" "$file" "$hPATH/${filename}.csv" || die "Upload failed for $filename."
 #       refresh "$tbl" die
    else
        warn "File '$file' does not exist."
    fi
}

# Install Cloudera Hadoop (Impala) connector, MDS, and FastTree dependencies in GITLAB repository
function install() {
    local libp=$1
    libp=$(dirname "$libp")
    [ ! -d "$libp" ] && mkdir "$libp"

    cd "$libp" || exit 1
    for repo in hadoopBySSH distance convert phylo; do
        if [ ! -d "$repo" ]; then
            git clone git@git.biotech.cdc.gov:vfn4/${repo}.git > /dev/null || die "Failed to install $repo"
        else
            cd "$libp/$repo" || exit 1
            git pull || die "Failed to pull data for $repo."
            cd "$libp" || exit 1
        fi
    done
}

function refresh() {
    local tbl=$1
    local fnc=$2
    time_stamp "Refreshing '$tbl'"
    "$libp/hadoopBySSH/himpala" "refresh ${hDB}.$tbl; compute stats ${hDB}.$tbl;" > /dev/null || $fnc "Refresh of '$tbl' failed."
}

### INSTALL DEPENDENCIES ###
# deprecated
if [ "$1" == "install" ]; then
    time_stamp "BEGIN"
    install "$libp"
    for i in bin docs lib r rmd share sql tbl viz; do
        [ ! -d "$bpath/$i" ] && mkdir "$bpath/$i"
    done
    time_stamp "END"
    echo ""
    exit 0
fi

### EXECUTE PROCESSES ###
# 1: DO
# uploads table for appsquared
if [ "$1" == "do" ]; then

    # 1-A: Upload reference files to Cloudera Hadoop schema
    time_stamp "BEGIN"
    upload_table "*.csv" glycosylation_distance || die
    #upload_table "$tblp/rank_limits.txt" rank_limits || die

    # 1-B: Refresh tables in Cloudera Hadoop 'protein_models' schema
    # NOTE: Strict order maintenance is required; several downstream tables depend on updates in prior tables (as indicated)
    hadoop_tables=(

        # tables
        #atomic_contacts
        glycosylation_distance
        #molecular_bonds
        #relative_site_interactions
        #rosetta_model
        #site_interactions

    )

    #source "$bpath/lib/ifx-shell/utils.sh"
'''
     1-C: Insert and optimize all provided SQL scripts into the CDP Cloudera Hadoop 'ref_sys' schema
    for tbl in "${hadoop_tables[@]}"; do
        time_stamp "Inserting table '$tbl'"
        "$libp/hadoopBySSH/himpala" "$sqlp/${tbl}.sql" || die "Insert of '$tbl' failed. Exiting."
    done

    time_stamp "Optmizing tables."
    "$libp/hadoopBySSH/himpala" "$sqlp/compute_stats.sql" >/dev/null || die "Compute stats failed."
'''
# 2: INSTALL
# Setup required direcotry/file architecture

elif [ "$1" == "install" ]; then
    time_stamp "BEGIN"
    install "$libp"

    for i in bin lib mds nwk share sql tbl viz; do
        [ ! -d "$bpath/$i" ] && mkdir "$bpath/$i"
    done
    time_stamp "END"

# 3: COMPUTE STATISTICS
# Run ad-hoc non-inferiority statistics

#elif [ "$1" == "stats" ]; then
    
    # 3A: Download data matrices for statistical metrics/computations

 #   time_stamp "BEGIN"
#    sql_query=(

        # 3A-1: Extract HI/HINT responses for the current influenza season (e.g., 2023SH) "reference" strains
   #     prod_reference_compstats

        # 3A-2: Extract HI/HINT responses for all "reference" strains in the current influenza season (e.g., 2023SH) HI/HINT panel
    #    prod_seasonal_compstats
   # )

    #for query in "${sql_query[@]}"; do
     #   bash "$libp/hadoopBySSH/himpala" "$sqlp/${query}.sql" > "$tblp/${query}.txt" || die
    #done

    # 3B: Execute R code to perform non-inferiority computations
    
  #  for query in "${sql_query[@]}"; do
      #  context=$( echo ${query} | cut -d _ -f2 )
     #   Rscript --vanilla "$rp/compStats.R" "$tblp/${query}.txt" "$tblp/${context}" "${context}"

    #    panels=(
   #         antigen
  #          antiserum
 #       )

#        for panel in "${panels[@]}"; do

            # 3B-1: Stage R output for CDP 'ref_sys' schema upload
   #         awk 'BEGIN{FS=OFS="\t"} {gsub(/\|/, "\t", $3)} 1' "$tblp/${context}_${panel}_NIanalysis.txt" > "$tblp/CDPupload_${context}_${panel}_NIanalysis.txt"
        
            # 3B-2: Upload staged TXT files to CDP 'ref_sys.noninferiority' schema/table
    #        upload_table "$tblp/CDPupload_${context}_${panel}_NIanalysis.txt" noninferiority || die
    #    done
   # done

#    time_stamp "END"

# 4: REFRESH STATISTICS ON CDP
# Refresh/update ad-hoc non-inferiority statistics
#elif [ "$1" == "update" ]; then
 #   contexts=(
  #      reference_antigen
   #     seasonal_antigen
    #    reference_antiserum
     #   seasonal_antiserum
  #  )

    # 4A: Upload all staged, archived TXT files to ref_sys.noninferiority schema/table
   # for context in "${contexts[@]}"; do
    #    upload_table "$tblp/CDPupload_${context}_NIanalysis.txt" noninferiority || die
  #  done

#else
 #   print_help
  #  time_stamp "Review input commands. Entry invalid."
#fi

finish "END"

